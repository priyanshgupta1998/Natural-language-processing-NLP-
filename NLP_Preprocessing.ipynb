{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshgupta1998/Natural-language-processing-NLP-/blob/master/NLP_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW3aKkzweVI2",
        "colab_type": "text"
      },
      "source": [
        "#NLTK , STOPWORDS REMOVING , TEXT PROCESSING , SpaCy library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeQKoOWRlcw5",
        "colab_type": "text"
      },
      "source": [
        "#popular NLTK, spaCy and Gensim libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVDnthFmk6x_",
        "colab_type": "text"
      },
      "source": [
        "#Remove Stopwords\n",
        "`We can remove stopwords while performing the following tasks:`\n",
        "\n",
        "# Text Classification\n",
        "  * Spam Filtering\n",
        "  * Language Classification\n",
        "  * Genre Classification\n",
        "* Caption Generation\n",
        "* Auto-Tag Generation\n",
        " \n",
        "\n",
        "#Avoid Stopword Removal\n",
        "* Machine Translation\n",
        "* Language Modeling\n",
        "* Text Summarization\n",
        "* Question-Answering problems\n",
        "\n",
        "\n",
        "`On removing stopwords, dataset size decreases and the time to train the model also decreases.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS0VM6CJevxT",
        "colab_type": "text"
      },
      "source": [
        "#1. Stopwords Removal using NLTK\n",
        "`NLTK, or the Natural Language Toolkit, is a treasure trove of a library for text prepro`cessing. NLTK has a list of stopwords stored in 16 different languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qkjiRPYd6CK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "16e79352-59da-4f3d-c1d3-bf42152d4f57"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmr2uGpHe-63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "efde6a94-c37e-4e12-dca7-e97c982c15ad"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u5CnoN1e-3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "163cdc0a-7d09-4b80-f544-5bcd023ab52c"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "liss = set(stopwords.words('english'))\n",
        "print(\"Total Number of stopwords in English\" , len(liss))\n",
        "print(list(liss)[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of stopwords in English 179\n",
            "['through', 'themselves', 'that', 'under', 'theirs', 'won', 'to', 'there', 'hadn', 'for']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62NwYE7be-1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "84b44e5c-925d-4233-d949-f6a44fb3bceb"
      },
      "source": [
        "# importing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "# sample sentence\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "# set of stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# tokens of words  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "\n",
        "print(\" \".join(word_tokens)) \n",
        "print(\" \".join(filtered_sentence)) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He determined to drop his litigation with the monastry , and relinguish his claims to the wood-cuting and fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had indeed the vaguest idea where the wood and river in question were .\n",
            "He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts . He ready becuase rights become much less valuable , indeed vaguest idea wood river question .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxXds2Sj3zk",
        "colab_type": "text"
      },
      "source": [
        "`Stopwords doesn't remove the punctuation marks or newline characters. We will need to remove them manually.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yCCkj9VkFZP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#2. Stopword Removal using spaCy\n",
        "`spaCy is one of the most versatile and widely used libraries in NLP. We can quickly and efficiently remove stopwords from the given text using SpaCy. It has a list of its own stopwords that can be imported as STOP_WORDS from the spacy.lang.en.stop_words class.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Tvg3r7d6Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "print(token_list)\n",
        "print(filtered_sentence) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DU-F2EZjjnM",
        "colab_type": "text"
      },
      "source": [
        "#3. Stopword Removal using Gensim\n",
        "`Gensim is a pretty handy library to work with on NLP tasks. While pre-processing, gensim provides methods to remove stopwords as well. We can easily import the remove_stopwords method from the class gensim.parsing.preprocessing.`\n",
        "\n",
        "`While using gensim for removing stopwords, we can directly use it on the raw text. There’s no need to perform tokenization before removing stopwords. `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAHeFH_5d6Gn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "593ca08a-509d-4b2d-9db9-3844a677aa11"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# pass the sentence in the remove_stopwords function\n",
        "result = remove_stopwords(\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, \n",
        "and he had indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "\n",
        "print(result)  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once. He ready becuase rights valuable, vaguest idea wood river question were.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC1TPpLTiol0",
        "colab_type": "text"
      },
      "source": [
        "#Text Normalization\n",
        "\n",
        "`we can easily understanding by seeing these words  ‘ate’, ‘eat’, or ‘eaten’ , that what's going on. But Unfortunately, that is not the case with machines. They treat these words differently. Therefore, we need to normalize them to their root word, which is “eat” in our example.`\n",
        "\n",
        "\n",
        "`Hence, text normalization is a process of transforming a word into a single canonical form. This can be done by two processes, stemming and lemmatization.`\n",
        "\n",
        "`which means reducing a word to its root form.`\n",
        "* Stemming --> It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
        "\n",
        "* Lemmatizer --> Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2togNUOifo7",
        "colab_type": "text"
      },
      "source": [
        "`Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.`\n",
        "\n",
        "`Lemmatization returns the lemma, which is the root word of all its inflection forms.`\n",
        "\n",
        "`We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8kpLm_qedSB",
        "colab_type": "text"
      },
      "source": [
        "#1. Text Normalization using NLTK\n",
        "`The NLTK library has a lot of amazing methods to perform different steps of data preprocessing. There are methods like PorterStemmer() and WordNetLemmatizer() to perform stemming and lemmatization, respectively.`\n",
        "\n",
        "# (i) stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJeqJtWzd6Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e1614660-7e93-49d9-e0cf-abc777da5ef4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "\n",
        "Stem_words = []\n",
        "ps =PorterStemmer()\n",
        "for w in filtered_sentence:\n",
        "    rootWord=ps.stem(w)\n",
        "    Stem_words.append(rootWord)\n",
        "print(filtered_sentence)\n",
        "print(Stem_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
            "['He', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'He', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYBXtJLLgS_-",
        "colab_type": "text"
      },
      "source": [
        "# (ii) Lemmatization \n",
        "`Here, v stands for verb, a stands for adjective and n stands for noun. The lemmatizer only lemmatizes those words which match the pos parameter of the lemmatize method.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RJzby_cgG8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b8de1fb-8c0d-4ec1-8ad6-e056c7435216"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jydGBWed6Ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a4ecc28f-6e0c-4ef3-bdd2-64a9c2d387bb"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(text) \n",
        "    \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "# print(filtered_sentence) \n",
        "\n",
        "lemma_word = []\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for w in filtered_sentence:\n",
        "    #Go through all process of the dictionary\n",
        "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")   #Search for Noun\n",
        "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")   #Search for Verb\n",
        "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))   #Search for Adjective\n",
        "    lemma_word.append(word3)   # Finally append the lemma of the original word\n",
        "print(lemma_word)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'le', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzAEMNHGgd86",
        "colab_type": "text"
      },
      "source": [
        "# 2. Text Normalization using spaCy\n",
        "`spaCy, is an amazing NLP library. It provides many industry-level methods to perform lemmatization. Unfortunately, spaCy has no module for stemming.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmPtyQOSgxsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c0ef0c11-cc5d-4e60-a28c-04773a9ff360"
      },
      "source": [
        "#download the english model\n",
        "!python -m spacy download en"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMJgEufXd6PR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "910a1077-0ec5-4b5f-a5b3-2ab73f7e4167"
      },
      "source": [
        "#make sure to download the english model with \"python -m spacy download en\"\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()   #Load english model in nlp variable\n",
        "\n",
        "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "print(type(doc))\n",
        "\n",
        "lemma_word1 = [] \n",
        "for token in doc:\n",
        "    lemma_word1.append(token.lemma_)\n",
        "print(lemma_word1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "['-PRON-', 'determine', 'to', 'drop', '-PRON-', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', '-PRON-', 'claim', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', '-PRON-', 'be', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'less', 'valuable', ',', 'and', '-PRON-', 'have', '\\n', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'be', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jElpbzNThTJf",
        "colab_type": "text"
      },
      "source": [
        "#3. Text Normalization using TextBlob\n",
        "`TextBlob is a Python library especially made for preprocessing text data. It is based on the NLTK library. We can use TextBlob to perform lemmatization. However, there’s no module for stemming in TextBlob.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3rjUWg3d6RP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8df3f34d-1697-4d98-ed05-1aa8ee18442a"
      },
      "source": [
        "from textblob import Word \n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "lem = []\n",
        "for i in text.split():\n",
        "    word1 = Word(i).lemmatize(\"n\")\n",
        "    word2 = Word(word1).lemmatize(\"v\")\n",
        "    word3 = Word(word2).lemmatize(\"a\")\n",
        "    lem.append(Word(word3).lemmatize())\n",
        "print(lem)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determine', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry,', 'and', 'relinguish', 'his', 'claim', 'to', 'the', 'wood-cuting', 'and', 'fishery', 'rihgts', 'at', 'once.', 'He', 'wa', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'le', 'valuable,', 'and', 'he', 'have', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCL369qd7WE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5Ab9xpqd7rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYrUhNvd7n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACqOesVd7lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTNsmUUd7jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofL4Z2Ud7hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOF-GojUd7fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TheH0UqGd7cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su90sO3Bd7Z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP4hAPHzd7TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}