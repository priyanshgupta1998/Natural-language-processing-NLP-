{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshgupta1998/Natural-language-processing-NLP-/blob/master/Essential_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz9CpXBMot94",
        "colab_type": "text"
      },
      "source": [
        "#All essential task in NLP from scratch\n",
        "\n",
        "* Stemming\n",
        "* Lemmatisation\n",
        "* Word Embeddings\n",
        "* Part-of-Speech Tagging (POS)\n",
        "* Named Entity Disambiguation\n",
        "* Named Entity Recognition\n",
        "* Sentiment Analysis\n",
        "* Semantic Text Similarity\n",
        "* Language Identification\n",
        "Text Summarisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cHAi3NPpLVy",
        "colab_type": "text"
      },
      "source": [
        "`Stemming`     \n",
        "`Stemming reduce the word lenght by choping off the suffix from the word , even if the stem is not a dictionary word. For example, in the English language-`  \n",
        "`Ex. 1. beautiful and beautifully are stemmed to beauti`        \n",
        "`Ex. 2.good, better and best are stemmed to good, better and best respectively.`\n",
        "\n",
        "           \n",
        "               \n",
        "\n",
        "`Lemmatization `        \n",
        "`Lemmatisation is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence , and the meaning of the word in the nearby sentences etc.`      \n",
        "`Ex. beautiful and beautifully are lemmatised to beautiful and beautifully respectively.`    \n",
        "`Ex. good, better and best are lemmatised to good, good and good respectively.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mticywAto7Wd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47f5aed7-6998-4123-c9b6-866ac2af0aca"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"multiplying\" \n",
        "\n",
        "print(stem.stem(word))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "multipli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BalM-p4RsuYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b006536f-c0b9-4731-87f1-4b393a29223b"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX3v707po8bN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d17110a0-0a96-4e63-ff25-cf8e173bbd9c"
      },
      "source": [
        "#unsing spaCy\n",
        "import spacy\n",
        "nlp=spacy.load(\"en\")\n",
        "doc=\"good better best\"\n",
        "\n",
        "for token in nlp(doc):\n",
        "    print(token , ' --> ',token.lemma_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good  -->  good\n",
            "better  -->  better\n",
            "best  -->  good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQdOE0ybtBVA",
        "colab_type": "text"
      },
      "source": [
        "#Word Embeddings\n",
        "`Word Embeddings is a techniques which are used to represent Natural Language in vector form of real numbers. Because computer only understand the numbers, it wouldn't undetstand the natural language.`\n",
        "\n",
        "\n",
        "`Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100.`\n",
        "\n",
        "[Great Visualization tool for word embedding](https://ronxin.github.io/wevi/) \n",
        "\n",
        "\n",
        "`We can obtain pre-trained Word Vector of a word using the gensim package.`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFMV1flDx1Jc",
        "colab_type": "text"
      },
      "source": [
        "#4. Part-Of-Speech Tagging\n",
        "`What is Part-Of-Speech Tagging?: In Simplistic terms, Part-Of-Speech Tagging is the process of marking up of words in a sentence as nouns, verbs, adjectives, adverbs etc. For example, in the sentence-`\n",
        "\n",
        "`“Ashok killed the snake with a stick”`\n",
        "\n",
        "`The Parts-Of-Speech are identified as –`\n",
        "\n",
        "      `Ashok PROPN`     \n",
        "      `killed VERB`     \n",
        "      `the DET`      \n",
        "      `snake NOUN`    \n",
        "      `with ADP`    \n",
        "      `a DET`      \n",
        "      `stick NOUN`    \n",
        "      `. PUNCT`    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWmiXTfZo81I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "c333fecc-b9c9-4da3-c070-7ca943589200"
      },
      "source": [
        "!python -m spacy download en \n",
        "nlp=spacy.load('en')\n",
        "sentence=\"Ashok killed the snake with a stick\"\n",
        "for token in nlp(sentence):\n",
        "   print(token,token.pos_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Ashok PROPN\n",
            "killed VERB\n",
            "the DET\n",
            "snake NOUN\n",
            "with ADP\n",
            "a DET\n",
            "stick NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNuh4ZDeyhYh",
        "colab_type": "text"
      },
      "source": [
        "#Named Entity Disambiguation\n",
        "\n",
        "`Named Entity Disambiguation is the process of identifying the mentions of entities in a sentence. or we can say it tells the difference of the meaning of a words in different-different places. For example, in the sentence-`\n",
        "\n",
        "`Ex. “Apple earned a revenue of 200 Billion USD in 2016”`\n",
        "\n",
        "`It is the task of Named Entity Disambiguation to infer that Apple in the sentence is the company Apple and not a fruit.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwlus77A0ktL",
        "colab_type": "text"
      },
      "source": [
        "#Named Entity Recognition\n",
        "\n",
        "`Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example,`\n",
        "\n",
        "`Ex. “Ram of Apple Inc. travelled to Sydney on 5th October 2017”`\n",
        "\n",
        "`and return something like`\n",
        "\n",
        "    `Ram `\n",
        "    `of`\n",
        "    `Apple ORG`\n",
        "    `Inc. ORG`\n",
        "    `travelled`\n",
        "    `to`\n",
        "    `Sydney GPE`\n",
        "    `on`\n",
        "    `5th DATE`\n",
        "    `October DATE`\n",
        "    `2017 DATE`\n",
        "\n",
        "`Here, ORG stands for Organisation and GPE stands for location.`\n",
        "\n",
        "![alt text](https://i0.wp.com/s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/26091557/ner.png?resize=604%2C269&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ogXFeUKo8ys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9ae7233d-18f0-48a3-dd66-108a629ace34"
      },
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en')\n",
        "sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"\n",
        "for token in nlp(sentence):\n",
        "   print(token, token.ent_type_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ram \n",
            "of \n",
            "Apple ORG\n",
            "Inc. ORG\n",
            "travelled \n",
            "to \n",
            "Sydney GPE\n",
            "on \n",
            "5th ORDINAL\n",
            "October DATE\n",
            "2017 DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwwTwTzs3JiQ",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "`Sentiment Analysis uses Natural Language processing techniques to perform tasks such as`\n",
        "* identifying the sentiment of a customer review, \n",
        "* positive or negative feeling in a sentence,\n",
        "* judging mood via voice analysis or written text analysis etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXOwXE-Q5NCc",
        "colab_type": "text"
      },
      "source": [
        "#Semantic Text Similarity\n",
        "`Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.For example –`\n",
        "\n",
        "`Ex. Car and Bus are similar but Car and fuel are related.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZfA-lTu6JC4",
        "colab_type": "text"
      },
      "source": [
        "#Text Summerization\n",
        "`Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ju0pAho8we",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2bb0e0e0-4123-4854-980f-a46e56dadde7"
      },
      "source": [
        "from gensim.summarization import summarize\n",
        "\n",
        "sentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"\n",
        "\n",
        "summarize(sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images.\\nExtractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5SbjKBKm7os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaNQFrWlm7nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwl1Hjg4m7lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZrqRW8Xm7kI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8kCPQurm7iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1we08Bjm7ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}