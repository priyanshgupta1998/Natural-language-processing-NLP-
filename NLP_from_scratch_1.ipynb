{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment23.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshgupta1998/Natural-language-processing-NLP-/blob/master/NLP_from_scratch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JfJNYlBpnbh",
        "colab_type": "text"
      },
      "source": [
        "### contractions module  is used  to replace contractions with their full forms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9xXLSLCmlcb",
        "colab_type": "code",
        "outputId": "14905f39-ba93-48ba-9704-2f7144f9c247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/67/f7/1462c6d28ec27ef2812aa2e9376c7fc7b39a23f0e02297f71119d74375c5/contractions-0.0.18-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1H3NQLHpJIz",
        "colab_type": "code",
        "outputId": "d941ba45-ed7f-4b47-9319-4c6a59fb4d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import contractions\n",
        "print(contractions.fix(\"don't\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "do not\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LOiXXyepTW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=\"Hi Steve  Thank you so much for reaching out and taking the time to send us feedback! It is very much appreciated! Please excuse the delayed response. Unfortunately, these scanning issues mostly occur due to older scanning technologies still used by some stores. Some devices (e.g. laser scanners or flatbed scanners) have difficulties reading smartphone displays. We hope to be able to solve this problem soon though as most stores will hopefully replace these scanning devices with newer image-based scanners in the course of their next cash system update. Until then, you can always ask the store staff to type in the card number manually in these cases so that you don't miss out on any rewards points or discounts. In the meantime, I sincerely apologize for the inconvenience this causes.  Once again, thank you for taking the time to contact us about this issue. If you have any further questions, suggestions for improvements or general feedback, please don't hesitate to contact us again.Best regards,Isabelle van Capelleveen Customer Support GmbH C-HUB / Hafenstraße 25-27 68159 Mannheim\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l25MF6SDmibm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#replace all the contractions\n",
        "data = contractions.fix(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htpnObJKmiZq",
        "colab_type": "code",
        "outputId": "e5c99d58-f8f8-4fed-a9c4-a7529905294a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Steve  Thank you so much for reaching out and taking the time to send us feedback! It is very much appreciated! Please excuse the delayed response. Unfortunately, these scanning issues mostly occur due to older scanning technologies still used by some stores. Some devices (e.g. laser scanners or flatbed scanners) have difficulties reading smartphone displays. We hope to be able to solve this problem soon though as most stores will hopefully replace these scanning devices with newer image-based scanners in the course of their next cash system update. Until then, you can always ask the store staff to type in the card number manually in these cases so that you do not miss out on any rewards points or discounts. In the meantime, I sincerely apologize for the inconvenience this causes.  Once again, thank you for taking the time to contact us about this issue. If you have any further questions, suggestions for improvements or general feedback, please do not hesitate to contact us again.Best regards,Isabelle van Capelleveen Customer Support GmbH C-HUB / Hafenstraße 25-27 68159 Mannheim'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49zhzUk5t59r",
        "colab_type": "text"
      },
      "source": [
        "# Now to get frquency of important words we need to do stemming as same word can have different representation in diff context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8cBssVmiXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "sbEng = SnowballStemmer('english')\n",
        "sbEsp = SnowballStemmer('spanish')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkJqFGOmmiVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in (data).split(' '):\n",
        "    print (sbEng.stem(item))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6KFKeNmiUJ",
        "colab_type": "code",
        "outputId": "380be022-d363-4423-e48d-f5dfeb79ceb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data1 = ' '.join([sbEng.stem(item) for item in (data).split(' ')])\n",
        "data1   #after stemming and removing contraction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi steve  thank you so much for reach out and take the time to send us feedback! it is veri much appreciated! pleas excus the delay response. unfortunately, these scan issu most occur due to older scan technolog still use by some stores. some devic (e.g. laser scanner or flatb scanners) have difficulti read smartphon displays. we hope to be abl to solv this problem soon though as most store will hope replac these scan devic with newer image-bas scanner in the cours of their next cash system update. until then, you can alway ask the store staff to type in the card number manual in these case so that you do not miss out on ani reward point or discounts. in the meantime, i sincer apolog for the inconveni this causes.  onc again, thank you for take the time to contact us about this issue. if you have ani further questions, suggest for improv or general feedback, pleas do not hesit to contact us again.best regards,isabell van capelleveen custom support gmbh c-hub / hafenstraß 25-27 68159 mannheim'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5dSm6iFmiSX",
        "colab_type": "code",
        "outputId": "5cc1775a-dfff-427d-d69e-20fcb51fd4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data) , len(data1)    #92 words were redundant"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1098, 1006)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vugLx8sEmiQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming only performed cutting prefix or suffix sometimes thats not useful \n",
        "# So, lemmatization uses morphological analysis too over words\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7tjrGbvJnL",
        "colab_type": "code",
        "outputId": "bfce7e2d-daa4-4e85-e222-fb6842bf6772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lancaster_stemmer.stem(\"time\") , lancaster_stemmer.stem(\"morning\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tim', 'morn')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJQhao6unIu",
        "colab_type": "code",
        "outputId": "1e0e74f0-8ddd-4536-cfda-c1363955882c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data2=' '.join(lancaster_stemmer.stem(item) for item in data.split(\" \"))\n",
        "data2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi stev  thank you so much for reach out and tak the tim to send us feedback! it is very much appreciated! pleas excus the delay response. unfortunately, thes scan issu most occ due to old scan technolog stil us by som stores. som dev (e.g. las scan or flatb scanners) hav difficul read smartphon displays. we hop to be abl to solv thi problem soon though as most stor wil hop replac thes scan dev with new image-based scan in the cours of their next cash system update. until then, you can alway ask the stor staff to typ in the card numb man in thes cas so that you do not miss out on any reward point or discounts. in the meantime, i sint apolog for the inconveny thi causes.  ont again, thank you for tak the tim to contact us about thi issue. if you hav any furth questions, suggest for improv or gen feedback, pleas do not hesit to contact us again.best regards,isabelle van capelleveen custom support gmbh c-hub / hafenstraß 25-27 68159 mannheim'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnZVUWPEunLw",
        "colab_type": "code",
        "outputId": "446a27a4-0146-4d8f-916a-df9d0c387bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "952"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeLJaujLyHzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone -b development https://github.com/clips/pattern\n",
        "# !pip install pattern\n",
        "# import pattern\n",
        "# dir(pattern)\n",
        "# !pip install Pattern3\n",
        "# !pip install pattern3\n",
        "# from pattern.en import tag\n",
        "# import pattern.en as lemEng\n",
        "# import pattern.es as lemEsp #for spanish words    \n",
        "# # this is not supported in python3 thats why not running this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y29IUjnzRNvE",
        "colab_type": "text"
      },
      "source": [
        "#### There is a multi-language package called cucco that can do all of the most common normalisation tasks in English, Spanish and about 10 other languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed2l4sXbRZNb",
        "colab_type": "code",
        "outputId": "3cf71c35-dfed-4e47-9218-ccf12150b32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install cucco"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cucco\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/a4/885b87757c0dabb87b6fa7f7ea214f33c03d6159aa4bcd7d7024f07839e5/cucco-2.2.1.tar.gz (51kB)\n",
            "\r\u001b[K     |██████▍                         | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: cucco\n",
            "  Building wheel for cucco (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/9f/88/4601c19912235677fd8fa8a2958cae82dcc63a1d8672633e29\n",
            "Successfully built cucco\n",
            "Installing collected packages: cucco\n",
            "Successfully installed cucco-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6vIUk-ZyHxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We need to remove the punctuation marks and all of that extra whitespace. Another thing we want to get rid of are non-signal, or stop words, that are likely to be common across texts, such as ‘a’, ‘the’, and ‘in’. These tasks fall into a process called normalisation.\n",
        "\n",
        "from cucco import Cucco\n",
        "\n",
        "normEng = Cucco(language='en')\n",
        "normEsp = Cucco(language='es') # for spanish"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY66Fpgkao4q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEYCBUnkmiM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3OmVhORSiLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMV2Hb3Sj6D",
        "colab_type": "text"
      },
      "source": [
        "#Preparing Text for Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjN5YmBXS9GN",
        "colab_type": "text"
      },
      "source": [
        "The raw text is unstructured in nature. Unstructured information has text , dates , numbers and facts. These make for irregularities and ambiguity which makes it difficult for the machine to understand the raw text.     \n",
        "\n",
        "In order for machine to be able to deal with text data , the text data needs to be first cleaned and prepared so that it can be fed to the Machine Learning Algorithm for analysis.   \n",
        "\n",
        "Text-clearning steps   \n",
        "Step 1 : load the text.    \n",
        "Step 2 : Split the text into tokens — -> it could be words , sentence or even paragraphs.    \n",
        "Step 3 : We now need to convert all the words in to its lower case because computer reads Man and man differently.     \n",
        "Step 4 : Remove the punctuation from each tokens.    \n",
        "Step 5 : Filter the remaining tokens that are not alphabetic     \n",
        "Step 6 : remove the stop words.    \n",
        "Step 7 : We can reduce the word to its root example fished , fisher all stem to fish.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsCiqQTUBdI",
        "colab_type": "text"
      },
      "source": [
        "Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers.  \n",
        "One of the easiest and most commonly used method of Feature Extraction is Bag of Words Model (BoW). (Frequncy of the individual word in the document)\n",
        "\n",
        "Bag Of Words : A Bag of Words represents a text that describes the occurrence of the words within the documents.   The model is only concerned with the presence of word in the document.After all the text cleaning we take the unique words only to form the vocabulary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP5wry0GmiLS",
        "colab_type": "code",
        "outputId": "7c336393-3668-4d14-b2e0-95389505b154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "##This is our initial corpus , We want to create a Dictionary out of it\n",
        "data = '''Jack and Jill \n",
        "Went up the hill\n",
        "To fetch a pail of water\n",
        "Jack fell down\n",
        "And broke his crown,\n",
        "And Jill came tumbling after.'''\n",
        "\n",
        "#Create a list of words from the data.\n",
        "data = data.split()\n",
        "print(len(data) , data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25 ['Jack', 'and', 'Jill', 'Went', 'up', 'the', 'hill', 'To', 'fetch', 'a', 'pail', 'of', 'water', 'Jack', 'fell', 'down', 'And', 'broke', 'his', 'crown,', 'And', 'Jill', 'came', 'tumbling', 'after.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaY88BBnmiJA",
        "colab_type": "code",
        "outputId": "ecb9bfe9-e631-4f42-f265-d5213ce42184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Take unidue words\n",
        "Dictionary = set(data)\n",
        "print( len(Dictionary) , Dictionary )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22 {'And', 'of', 'hill', 'and', 'down', 'Jill', 'fetch', 'up', 'came', 'crown,', 'broke', 'a', 'the', 'his', 'tumbling', 'water', 'To', 'pail', 'after.', 'Jack', 'fell', 'Went'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBW8N2WndGs0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Therefore there is a need to reduce the vocabulary size     \n",
        ">Ignoring case.   \n",
        "Ignoring punctuation.   \n",
        "Ignoring frequent words that don’t contain much information, called stop words, like a, of,etc.   \n",
        "Fixing misspelled words.   \n",
        "Reducing words to their stem (e.g. play from playing) using stemming algorithms.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OoOycIpSmY8",
        "colab_type": "code",
        "outputId": "e66bb520-ba21-47a3-9c23-1562657fccae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "n_gram = ngrams(Dictionary,2)   #Bi-gram\n",
        "n_gram"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object ngrams at 0x7f4f9bbaf9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdhSNgIbSmXD",
        "colab_type": "code",
        "outputId": "ca329c40-e29e-411a-b647-034afd436355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "bigrams = list(Counter(n_gram))\n",
        "bigrams "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('And', 'of'),\n",
              " ('of', 'hill'),\n",
              " ('hill', 'and'),\n",
              " ('and', 'down'),\n",
              " ('down', 'Jill'),\n",
              " ('Jill', 'fetch'),\n",
              " ('fetch', 'up'),\n",
              " ('up', 'came'),\n",
              " ('came', 'crown,'),\n",
              " ('crown,', 'broke'),\n",
              " ('broke', 'a'),\n",
              " ('a', 'the'),\n",
              " ('the', 'his'),\n",
              " ('his', 'tumbling'),\n",
              " ('tumbling', 'water'),\n",
              " ('water', 'To'),\n",
              " ('To', 'pail'),\n",
              " ('pail', 'after.'),\n",
              " ('after.', 'Jack'),\n",
              " ('Jack', 'fell'),\n",
              " ('fell', 'Went')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOyiL_y3fAuX",
        "colab_type": "code",
        "outputId": "d7132de6-d3fe-46d5-e64c-245962510d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(bigrams)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7h-ZpKMew4l",
        "colab_type": "code",
        "outputId": "a9752b15-e1fa-492a-91cb-01fce52a4856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_gram = ngrams(Dictionary,3)   #tri-gram\n",
        "n_gram"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object ngrams at 0x7f4f9bc7fa40>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FREmrm02e1yC",
        "colab_type": "code",
        "outputId": "6a976dc0-189f-4368-d2ba-09f2eacf7113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "trigrams = list(Counter(n_gram))\n",
        "trigrams "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('And', 'of', 'hill'),\n",
              " ('of', 'hill', 'and'),\n",
              " ('hill', 'and', 'down'),\n",
              " ('and', 'down', 'Jill'),\n",
              " ('down', 'Jill', 'fetch'),\n",
              " ('Jill', 'fetch', 'up'),\n",
              " ('fetch', 'up', 'came'),\n",
              " ('up', 'came', 'crown,'),\n",
              " ('came', 'crown,', 'broke'),\n",
              " ('crown,', 'broke', 'a'),\n",
              " ('broke', 'a', 'the'),\n",
              " ('a', 'the', 'his'),\n",
              " ('the', 'his', 'tumbling'),\n",
              " ('his', 'tumbling', 'water'),\n",
              " ('tumbling', 'water', 'To'),\n",
              " ('water', 'To', 'pail'),\n",
              " ('To', 'pail', 'after.'),\n",
              " ('pail', 'after.', 'Jack'),\n",
              " ('after.', 'Jack', 'fell'),\n",
              " ('Jack', 'fell', 'Went')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN7gCJawe8mg",
        "colab_type": "code",
        "outputId": "b01eb708-b7d4-40ee-e3ac-96e1b6f3d570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(trigrams)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Pvu3edeX9o",
        "colab_type": "text"
      },
      "source": [
        "Creating a Bigram or trigram would effectively reduce the size of the vocabulary also. and allows the bag of words to capture a little bit more meaning from the document. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbyOqVONfUq3",
        "colab_type": "text"
      },
      "source": [
        "Natural Language Processing is a sub field of artificial-intelligence that helps in processing and analyzing natural language like text,speech and so on.\n",
        "\n",
        "We can divide whole NLP pipeline into 3 parts\n",
        "\n",
        ">Text preprocessing  \n",
        "Feature engineering  \n",
        "Model building and evaluation    \n",
        "\n",
        ">>>>![alt text](https://cdn-images-1.medium.com/max/800/1*zfGwHSTGqbaFwb_O8KQrnQ.png)\n",
        "\n",
        "\n",
        "#Text Preprocessing\n",
        "Text is mostly in unstructured form. Lot of noises will be present in it.In data preprocessing we will remove the noises associated with it.\n",
        "\n",
        "###Steps in text cleaning:\n",
        "\n",
        "1. Noise entity removal\n",
        "2. Text Normalization\n",
        "3. Word standardization\n",
        "\n",
        "`First of all we have to tokenize our data/text which is an important aspect in NLP. \n",
        "then We use nltk library to remove stop words and re library to remove punctuation's, numbers and white spaces.`\n",
        "\n",
        "Tokenization is the process of converting text to tokens.Tokens can be sentences or words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ZLTZt-hxA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEpIym-7h2dc",
        "colab_type": "code",
        "outputId": "ac1bf637-eb60-4cac-9b58-ab5eb1fee82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ7ynUH5hcJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = \"\"\"I have to  thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are. Making The Revenant was aboutman's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history#.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohoxH0eWhldc",
        "colab_type": "code",
        "outputId": "f3835f47-2fc3-4b4a-ad7c-6ef331b040d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "sentences = nltk.sent_tokenize(dataset)\n",
        "print(\"Sentence tokenize : \")\n",
        "print(len(sentences) , sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence tokenize : \n",
            "4 ['I have to  thank everyone from the very onset of my career … To my parents; none of this would be possible without you.', 'And to my friends, I love you dearly; you know who you are.', \"Making The Revenant was aboutman's relationship to the natural world.\", 'A world that we collectively felt in 2015 as the hottest year in recorded history#.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-YFj3jAiJd8",
        "colab_type": "code",
        "outputId": "59ffb258-425c-4f18-b30c-c12dfdb3865d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "words = nltk.word_tokenize(dataset)\n",
        "print(\"Word tokenize : \")\n",
        "print(len(words) , words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word tokenize : \n",
            "71 ['I', 'have', 'to', 'thank', 'everyone', 'from', 'the', 'very', 'onset', 'of', 'my', 'career', '…', 'To', 'my', 'parents', ';', 'none', 'of', 'this', 'would', 'be', 'possible', 'without', 'you', '.', 'And', 'to', 'my', 'friends', ',', 'I', 'love', 'you', 'dearly', ';', 'you', 'know', 'who', 'you', 'are', '.', 'Making', 'The', 'Revenant', 'was', 'aboutman', \"'s\", 'relationship', 'to', 'the', 'natural', 'world', '.', 'A', 'world', 'that', 'we', 'collectively', 'felt', 'in', '2015', 'as', 'the', 'hottest', 'year', 'in', 'recorded', 'history', '#', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfoL78nSi22J",
        "colab_type": "text"
      },
      "source": [
        "### Noise removing step "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmDnQURDjqHg",
        "colab_type": "code",
        "outputId": "682e4c86-67c8-49eb-b206-3c098ca9e031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYf6Cvf-fULi",
        "colab_type": "code",
        "outputId": "a78aa042-9313-405e-945b-2c313edc52e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "dataset = nltk.sent_tokenize(dataset)\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset[i] = dataset[i].lower()\n",
        "    dataset[i] = re.sub(r'\\W',' ',dataset[i])# will remove non-word charecters like #,*,% etc\n",
        "    dataset[i] = re.sub(r'\\d',' ',dataset[i])#will remove digits\n",
        "    dataset[i] = re.sub(r'\\s+',' ',dataset[i])#will remove extra spaces\n",
        "    words = nltk.word_tokenize(dataset[i])\n",
        "    new = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new.append(word)\n",
        "    dataset[i] = ' '.join(new)\n",
        "print(len(dataset) , dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 ['thank everyone onset career parents none would possible without', 'friends love dearly know', 'making revenant aboutman relationship natural world', 'world collectively felt hottest year recorded history']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyL4ctQlkeTt",
        "colab_type": "text"
      },
      "source": [
        "##2. Text Normalization: \n",
        "\n",
        "Another type of noise is the repetitions by single word multiple times.for example run,running,runs etc are different variations of term run. Normalization will helps reduce such words to a single word.Thus helps in reducing the dimension.     \n",
        "There are mainly two normalization techniques.   \n",
        " 1. Stemming:\n",
        "Stemming is a rule based approach which strips suffixes(ing,ly,s etc). some of the examples are:\n",
        "\n",
        ">>>![alt text](https://cdn-images-1.medium.com/max/800/1*xY0AT_nF3hO7ukQARRMRzw.png)\n",
        "\n",
        "There are many algorithms that will do stemming. but the most common in English is Porter stemmer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePHI-UX9SmVS",
        "colab_type": "code",
        "outputId": "d9679a51-512d-4cd5-c62a-f4458517fe58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    words = nltk.word_tokenize(dataset[i])\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    dataset[i] = ' '.join(words)\n",
        "print(len(dataset) , dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 ['thank everyon onset career parent none would possibl without', 'friend love dearli know', 'make reven aboutman relationship natur world', 'world collect felt hottest year record histori']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HVNj63Rl9Es",
        "colab_type": "text"
      },
      "source": [
        " 2. Lemmatization: Like stemming Lemmatization will also convert word to its root form.It is a step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).     \n",
        " \n",
        " The key to this method is linguistics.In lemmatization root word is called lemma.    \n",
        " \n",
        " The output of stemmers can be meaningless, but the output of lemmatizers will always be meaningful.some of exmaples of lemmatization of words are:   \n",
        " \n",
        " >>> ![alt text](https://cdn-images-1.medium.com/max/800/1*oWhtVUulX7Wnz-ljv25Yzg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPLJSQPRmc57",
        "colab_type": "code",
        "outputId": "8dcc5f1a-9aaf-43a4-9608-ddfe93ec6297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I3PgRBUSmTr",
        "colab_type": "code",
        "outputId": "90815797-24aa-4f4f-93df-c471228b7481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#AGAIN REFRESH THE 'Dataset' variable to perform this action and get result\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer()  #lemma object\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    words = nltk.word_tokenize(dataset[i])\n",
        "    words = [lem.lemmatize(word,pos='v') for word in words]\n",
        "    dataset[i] = ' '.join(words)\n",
        "print(len(dataset) , dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 ['thank everyone onset career parent none would possible without', 'friends love dearly know', 'make revenant aboutman relationship natural world', 'world collectively felt hottest year record history']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHcHSxCFnMCS",
        "colab_type": "text"
      },
      "source": [
        "##3. Word standardization:\n",
        "\n",
        "Text may contains words that are not in dictionary.for example in tweets or comments ,    \n",
        "it can contain words like ‘re’ representing are,’s’ for is,’awsm’ for awesome and so on.such words will not recognized by our model.so we have to fix it.    \n",
        "we will perform  it by using a tweet from twitter.We will create a lookup table for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6N-BXJ4SmRx",
        "colab_type": "code",
        "outputId": "6ad06fcd-a69f-4bfc-f226-2315fdbb42b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet = 'its an awsm day and my friends re superb'\n",
        "lookup = {'s': 'is', 're':'are', 'awsm': 'awesome', 'superb': 'super'}\n",
        "data = []\n",
        "\n",
        "for word in tweet.split():\n",
        "    if word in lookup.keys():\n",
        "        word = lookup[word]\n",
        "    data.append(word)\n",
        "tweet = ' '.join(data)\n",
        "print(len(tweet) , tweet)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43 its an awesome day and my friends are super\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNXldfB_o3gi",
        "colab_type": "text"
      },
      "source": [
        "#Feature Engineering\n",
        "To analyse a preprocessed data, it needs to be converted into features.   \n",
        "Inorder to convert it in to features we use differnet techniques like bag of words model, ngrams-model, tf-idf, word2vector and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZlcBBCgpKca",
        "colab_type": "text"
      },
      "source": [
        " 1. Bag of words model:  Convert text into vetors ( in nuemerical formet) . that contains the frequency of the word.  \n",
        "   > Consider the following sentences as reviews about a movie.   \n",
        "           'Movie is good and movie is worth watch'   \n",
        "            'Movie is average but story is really good'   \n",
        "            'I like the movie and the fight'   \n",
        "   >After data-preprocessing (Lower words)    \n",
        "           'movie good movie worth watch'   \n",
        "            'movie average story really good'   \n",
        "            'i like movie fight.'    \n",
        "    >   The three reviews can be represented as a collection of words. (Make lists)     \n",
        "        ['movie', 'good', 'movie', 'worth', 'watch']     \n",
        "        ['movie', 'average', 'story', 'really', 'good']    \n",
        "        ['I', 'like', 'movie', 'fight']      \n",
        "     >  tokens(unique words)    \n",
        "         'movie', 'good', 'worth', 'watch','average', 'story', 'really', 'I', 'like', 'fight'\n",
        "     > Bag of words  Technique        \n",
        "     'movie good worth watch'' =  [2,1,1,1,0,0,0,0,0,0]       \n",
        "     'movie average story really good' =  [1,1,0,0,1,1,1,0,0,0]       \n",
        "     'I like movie fight' = [1,0,0,0,0,0,0,1,1,1]     \n",
        "     \n",
        "In this approach, each word or token is called a “gram”.      \n",
        "A big document where the generated vocabulary is huge may result in a vector with lots of 0 values. This is called a sparse vector.    \n",
        "Sparse vectors require more memory and computational resources when modeling.    \n",
        "Sparse vectors together combine to form Sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXLG9FxPnQ8d",
        "colab_type": "code",
        "outputId": "dcb3377f-01e3-4387-acca-f55e8b4160bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data = ['movie good movie worth watch',\n",
        " 'movie average story really good',\n",
        " 'I like movie fight']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer(ngram_range=(0,1)) #in scikit-learn\n",
        "final_counts = count_vect.fit_transform(data)\n",
        "print(count_vect.get_feature_names())\n",
        "print(final_counts.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['average', 'fight', 'good', 'like', 'movie', 'really', 'story', 'watch', 'worth']\n",
            "[[0 0 1 0 2 0 0 1 1]\n",
            " [1 0 1 0 1 1 1 0 0]\n",
            " [0 1 0 1 1 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrvliGBetRfK",
        "colab_type": "text"
      },
      "source": [
        "### Limitation of Bag of words \n",
        "\n",
        "While modeling phrases using bag-of-words the order of words in the phrase is not respected. Ex: “This is bad” and “Is this bad” have exactly the same vector representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpqBIZStZ7z",
        "colab_type": "text"
      },
      "source": [
        "#TF-IDF Vectorizer   \n",
        "get the frequency in the ratio ( value in range 0 to 1 )\n",
        "TF-IDF stands for term frequency-inverse document frequency.   \n",
        "TF-IDF value is obtained by multiplying TF score and IDF score.\n",
        "\n",
        "##Term Frequency(TF):\n",
        "\n",
        "Term frequency of a word is the frequency of the word in the document. The term frequency is often divided by the document length to normalize.\n",
        "\n",
        ">>![alt text](https://cdn-images-1.medium.com/max/800/1*dDzcxSVVSu2tTh_J9i2uLg.png)\n",
        "\n",
        "\n",
        "##Inverse Document Frequency (IDF):     \n",
        "It reflects how important a word is to a document in a collection or corpus.      \n",
        "Find the number of documents that contains the particular word , which we are looking for .\n",
        "\n",
        ">>>![alt text](https://cdn-images-1.medium.com/max/800/1*78HrPv5j9mra-_3WpdNCSg.png)\n",
        "\n",
        "\n",
        "TfidfVectorizer is available in Sci-Kit learn library (sklearn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Z9oyVSnQ6V",
        "colab_type": "code",
        "outputId": "ab52dd7f-95bd-4603-c6c2-0a6a1b41d259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data = ['movie good movie worth watch',\n",
        " 'movie average story really good',\n",
        " 'I like movie fight']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vector = TfidfVectorizer()\n",
        "count = vector.fit_transform(data)\n",
        "print(vector.get_feature_names())\n",
        "print(count.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['average', 'fight', 'good', 'like', 'movie', 'really', 'story', 'watch', 'worth']\n",
            "[[0.         0.         0.38151877 0.         0.59256672 0.\n",
            "  0.         0.50165133 0.50165133]\n",
            " [0.50461134 0.         0.38376993 0.         0.29803159 0.50461134\n",
            "  0.50461134 0.         0.        ]\n",
            " [0.         0.65249088 0.         0.65249088 0.38537163 0.\n",
            "  0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6WdMRizwgPA",
        "colab_type": "text"
      },
      "source": [
        "**Limitations of TF- IDF model:**    \n",
        "\n",
        "TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text and semantics of words.   \n",
        "\n",
        "\n",
        "In order to solve this problem we can use word2vector model .\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4M3gmfrxIVe",
        "colab_type": "text"
      },
      "source": [
        "#Text processing problems with non-English languages  \n",
        "Take a look at the Croatian language (the same approach can be used for any other languages — Slovak, Finish, Turkish, etc). Since it is less widely spoken (in comparison to English, German, Spanish or French), it’s not practical to develop a new text processing tool for each of its language varieties. It would take too much time and too much data. Instead, it is better to find a way to standardize all the words in the observing text.\n",
        "\n",
        "The wrong meaning of one word in the sentence can totally change its’ sentiment.\n",
        "\n",
        "https://medium.com/krakensystems-blog/text-processing-problems-with-non-english-languages-82822d0945dd \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyfuc6GeSmAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}